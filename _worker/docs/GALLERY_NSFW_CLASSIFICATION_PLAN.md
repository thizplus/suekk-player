# Gallery NSFW Classification Plan

## Problem Statement

à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™à¸£à¸°à¸šà¸š Gallery Worker à¸ªà¸£à¹‰à¸²à¸‡ 100 à¸ à¸²à¸à¸ˆà¸²à¸ video à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸¡à¸µà¸à¸²à¸£à¸ˆà¸±à¸”à¸›à¸£à¸°à¹€à¸ à¸— (classification)

à¸—à¸³à¹ƒà¸«à¹‰à¹€à¸¡à¸·à¹ˆà¸­ SEO Worker à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸ à¸²à¸:
- à¸•à¹‰à¸­à¸‡ download à¸—à¸¸à¸à¸ à¸²à¸à¸¡à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š NSFW à¸—à¸µà¸¥à¸°à¸ à¸²à¸ (à¸Šà¹‰à¸²)
- à¸¡à¸±à¸à¸à¸šà¸§à¹ˆà¸² 94 à¸ à¸²à¸à¸ˆà¸²à¸ 100 à¸ à¸²à¸à¹€à¸›à¹‡à¸™ NSFW (à¹ƒà¸Šà¹‰à¹„à¸”à¹‰à¹à¸„à¹ˆ 6 à¸ à¸²à¸)
- à¸à¸¢à¸²à¸¢à¸²à¸¡ Smart Blur à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¸œà¹ˆà¸²à¸™ Falconsai classifier

## Proposed Solution

**à¹à¸à¹‰à¸—à¸µà¹ˆà¸•à¹‰à¸™à¸—à¸²à¸‡**: à¸—à¸³ NSFW classification à¸•à¸­à¸™à¸ªà¸£à¹‰à¸²à¸‡ gallery à¹€à¸¥à¸¢

### Phase 1: Add NSFW Classification to Gallery Worker

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Current Flow                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  HLS â†’ Extract 100 frames â†’ Upload ALL to S3 â†’ Done             â”‚
â”‚                                                                  â”‚
â”‚  Problem: SEO Worker à¸•à¹‰à¸­à¸‡ download + classify à¸—à¸µà¸«à¸¥à¸±à¸‡            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Proposed Flow                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  HLS â†’ Extract frame â†’ NSFW Check â†’ Classify as safe/nsfw       â”‚
â”‚                      â†“                                           â”‚
â”‚              Upload to separate folders:                         â”‚
â”‚              - gallery/{code}/safe/001.jpg                      â”‚
â”‚              - gallery/{code}/nsfw/001.jpg                      â”‚
â”‚                      â†“                                           â”‚
â”‚              Update DB with counts: safe_count, nsfw_count      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 2: Adaptive Frame Extraction (Multi-Round Strategy)

**à¸«à¸¥à¸±à¸à¸à¸²à¸£:** à¸–à¹‰à¸² Round à¹à¸£à¸à¹„à¸”à¹‰à¸ à¸²à¸ safe à¸™à¹‰à¸­à¸¢ â†’ à¸”à¸¶à¸‡à¸ à¸²à¸à¹€à¸à¸´à¹ˆà¸¡à¸ˆà¸²à¸à¸Šà¹ˆà¸§à¸‡à¹€à¸§à¸¥à¸²à¸—à¸µà¹ˆà¸•à¹ˆà¸²à¸‡à¸­à¸­à¸à¹„à¸›

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ROUND 1: Standard Extraction (100 frames)                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚  Timeline: [5%]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[95%]    â”‚
â”‚            â†“    â†“    â†“    â†“    â†“    â†“    â†“    â†“    â†“    â†“      â”‚
â”‚           f1   f10  f20  f30  f40  f50  f60  f70  f80  f100     â”‚
â”‚                                                                  â”‚
â”‚  Interval: (95% - 5%) / 100 = 0.9% per frame                    â”‚
â”‚  Result: safe_count = 6 (à¹„à¸¡à¹ˆà¸à¸­!)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ROUND 2: Intro Focus (20 frames)                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚  Timeline: [0%]â”€â”€â”€â”€â”€â”€â”€â”€[15%]                                    â”‚
â”‚            â†“  â†“  â†“  â†“  â†“  â†“  â†“  â†“  â†“  â†“                        â”‚
â”‚                                                                  â”‚
â”‚  à¹€à¸«à¸•à¸¸à¸œà¸¥: à¸Šà¹ˆà¸§à¸‡ intro à¸¡à¸±à¸à¹€à¸›à¹‡à¸™à¸à¸²à¸£à¸à¸¹à¸”à¸„à¸¸à¸¢, à¹à¸™à¸°à¸™à¸³à¸•à¸±à¸§ (safe)            â”‚
â”‚  Interval: 15% / 20 = 0.75% per frame                           â”‚
â”‚  Result: +4 safe images                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ROUND 3: Outro Focus (15 frames)                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚  Timeline:                              [90%]â”€â”€â”€â”€â”€â”€â”€â”€[100%]     â”‚
â”‚                                          â†“  â†“  â†“  â†“  â†“  â†“      â”‚
â”‚                                                                  â”‚
â”‚  à¹€à¸«à¸•à¸¸à¸œà¸¥: à¸Šà¹ˆà¸§à¸‡à¸—à¹‰à¸²à¸¢à¸¡à¸±à¸à¹€à¸›à¹‡à¸™ ending, credits (safe)                  â”‚
â”‚  Interval: 10% / 15 = 0.67% per frame                           â”‚
â”‚  Result: +2 safe images                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ROUND 4: Gap Fill (30 frames)                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚  Timeline: [5%]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[95%]    â”‚
â”‚              â†“   â†“   â†“   â†“   â†“   â†“   â†“   â†“   â†“   â†“             â”‚
â”‚            (offset +0.45% à¸ˆà¸²à¸ Round 1)                          â”‚
â”‚                                                                  â”‚
â”‚  à¹€à¸«à¸•à¸¸à¸œà¸¥: à¸”à¸¶à¸‡à¸ à¸²à¸à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ frame à¹€à¸”à¸´à¸¡ (à¸à¸¥à¸²à¸‡à¹† à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ Round 1)       â”‚
â”‚  Result: +3 safe images                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TOTAL: 6 + 4 + 2 + 3 = 15 safe images âœ“                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Algorithm:**

```go
type ExtractionRound struct {
    Name       string
    StartPct   float64  // % à¸‚à¸­à¸‡ video
    EndPct     float64
    FrameCount int
    Offset     float64  // offset à¸ˆà¸²à¸ interval à¸›à¸à¸•à¸´ (à¸ªà¸³à¸«à¸£à¸±à¸š gap fill)
}

var extractionRounds = []ExtractionRound{
    // Round 1: Standard (à¸à¸£à¸°à¸ˆà¸²à¸¢à¸—à¸±à¹‰à¸‡ video)
    {Name: "standard", StartPct: 0.05, EndPct: 0.95, FrameCount: 100, Offset: 0},

    // Round 2: Intro focus (0-15%)
    {Name: "intro", StartPct: 0.00, EndPct: 0.15, FrameCount: 20, Offset: 0},

    // Round 3: Outro focus (90-100%)
    {Name: "outro", StartPct: 0.90, EndPct: 1.00, FrameCount: 15, Offset: 0},

    // Round 4: Gap fill (à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ Round 1)
    {Name: "gap_fill", StartPct: 0.05, EndPct: 0.95, FrameCount: 30, Offset: 0.5},

    // Round 5: Dense intro (à¸–à¹‰à¸²à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸à¸­)
    {Name: "dense_intro", StartPct: 0.00, EndPct: 0.10, FrameCount: 30, Offset: 0.25},
}

func (h *GalleryHandler) extractWithRetry(ctx context.Context, job *models.GalleryJob) ([]ClassifiedImage, error) {
    var safeImages []ClassifiedImage
    var nsfwImages []ClassifiedImage
    minSafe := 12

    for _, round := range extractionRounds {
        if len(safeImages) >= minSafe {
            break // à¸à¸­à¹à¸¥à¹‰à¸§ à¸«à¸¢à¸¸à¸”à¹„à¸”à¹‰
        }

        h.logger.Info("extraction round",
            "round", round.Name,
            "current_safe", len(safeImages),
            "target", minSafe,
        )

        // Extract frames for this round
        frames := h.extractFrames(ctx, job, round)

        // Classify each frame
        for _, frame := range frames {
            result := h.classifyImage(ctx, frame)
            if result.IsSafe {
                safeImages = append(safeImages, frame)
            } else {
                nsfwImages = append(nsfwImages, frame)
            }
        }
    }

    h.logger.Info("extraction complete",
        "total_safe", len(safeImages),
        "total_nsfw", len(nsfwImages),
        "rounds_used", getRoundsUsed(len(safeImages), minSafe),
    )

    return safeImages, nil
}
```

**Timestamp Deduplication:**

```go
// à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸ à¸²à¸à¸‹à¹‰à¸³: track timestamps à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¹„à¸›à¹à¸¥à¹‰à¸§
type TimestampTracker struct {
    used      map[int]bool  // timestamp (seconds) à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¹à¸¥à¹‰à¸§
    minGap    int           // minimum gap between frames (seconds)
}

func (t *TimestampTracker) IsAvailable(timestamp float64) bool {
    sec := int(timestamp)
    // Check if any nearby timestamp was used
    for i := sec - t.minGap; i <= sec + t.minGap; i++ {
        if t.used[i] {
            return false
        }
    }
    return true
}

func (t *TimestampTracker) Mark(timestamp float64) {
    t.used[int(timestamp)] = true
}
```

### Phase 3: Smart Segment Detection (Future Enhancement)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  If still not enough after Round 5:                             â”‚
â”‚                                                                  â”‚
â”‚  Option A: Scene Detection                                      â”‚
â”‚    - à¹ƒà¸Šà¹‰ FFmpeg scene detection à¸«à¸² "talking scenes"             â”‚
â”‚    - à¸”à¸¶à¸‡à¸ à¸²à¸à¸ˆà¸²à¸à¸Šà¹ˆà¸§à¸‡à¸—à¸µà¹ˆà¸¡à¸µ scene change à¸™à¹‰à¸­à¸¢ (à¸¡à¸±à¸à¹€à¸›à¹‡à¸™à¸à¸²à¸£à¸à¸¹à¸”à¸„à¸¸à¸¢)    â”‚
â”‚                                                                  â”‚
â”‚  Option B: Audio Analysis                                       â”‚
â”‚    - à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ audio à¸«à¸²à¸Šà¹ˆà¸§à¸‡à¸—à¸µà¹ˆà¸¡à¸µà¹€à¸ªà¸µà¸¢à¸‡à¸à¸¹à¸” (speech)               â”‚
â”‚    - à¸”à¸¶à¸‡à¸ à¸²à¸à¸ˆà¸²à¸à¸Šà¹ˆà¸§à¸‡à¸—à¸µà¹ˆà¸¡à¸µ speech (à¸¡à¸±à¸à¹€à¸›à¹‡à¸™ safe)                   â”‚
â”‚                                                                  â”‚
â”‚  Option C: Accept Fewer Images                                  â”‚
â”‚    - à¸–à¹‰à¸² video à¹€à¸›à¹‡à¸™ NSFW à¹€à¸à¸·à¸­à¸šà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”                           â”‚
â”‚    - à¸¢à¸­à¸¡à¸£à¸±à¸š safe images à¸™à¹‰à¸­à¸¢à¸à¸§à¹ˆà¸² 12                             â”‚
â”‚    - Flag video à¸§à¹ˆà¸² "limited_gallery"                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## System Architecture: 2 Entry Points

**à¸ªà¸³à¸„à¸±à¸:** à¸£à¸°à¸šà¸šà¸¡à¸µ 2 à¸—à¸²à¸‡à¹ƒà¸™à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡ gallery à¸•à¹‰à¸­à¸‡à¹à¸à¹‰à¸—à¸±à¹‰à¸‡ 2 à¸ˆà¸¸à¸”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Entry Point 1: Auto-generation (Transcode)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Video Upload â†’ Transcode Worker â†’ generateAndUploadGallery()  â”‚
â”‚                                    â†“                            â”‚
â”‚                          transcoder/gallery.go                  â”‚
â”‚                          (à¹ƒà¸Šà¹‰ Local video file - à¹€à¸£à¹‡à¸§)          â”‚
â”‚                                    â†“                            â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚                          â”‚ NSFW Classifier â”‚ â† NEW              â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                    â†“                            â”‚
â”‚                          Upload safe/ + nsfw/ to S3             â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Entry Point 2: Manual Trigger (Frontend)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Frontend à¸à¸”à¸›à¸¸à¹ˆà¸¡ â†’ API â†’ PublishGalleryJob() â†’ NATS Queue       â”‚
â”‚                                                â†“                â”‚
â”‚                               gallery_handler.go:ProcessJob()   â”‚
â”‚                               (à¹ƒà¸Šà¹‰ HLS à¸ˆà¸²à¸ S3 - à¸Šà¹‰à¸²à¸à¸§à¹ˆà¸²)        â”‚
â”‚                                                â†“                â”‚
â”‚                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚                               â”‚ NSFW Classifier â”‚ â† NEW         â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                â†“                â”‚
â”‚                               Upload safe/ + nsfw/ to S3        â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Shared NSFW Classifier Module

à¸—à¸±à¹‰à¸‡ 2 path à¹ƒà¸Šà¹‰ classifier à¸£à¹ˆà¸§à¸¡à¸à¸±à¸™:

```
_worker/
â”œâ”€â”€ infrastructure/
â”‚   â””â”€â”€ classifier/              â† NEW: Shared module
â”‚       â”œâ”€â”€ nsfw_classifier.go   # Go wrapper (call Python)
â”‚       â”œâ”€â”€ classify_batch.py    # Python NudeNet + logic
â”‚       â””â”€â”€ types.go             # ClassificationResult struct
â”‚
â”œâ”€â”€ infrastructure/transcoder/
â”‚   â””â”€â”€ gallery.go               # Entry Point 1 (à¹à¸à¹‰à¹„à¸‚)
â”‚
â””â”€â”€ use_cases/
    â””â”€â”€ gallery_handler.go       # Entry Point 2 (à¹à¸à¹‰à¹„à¸‚)
```

### Integration Flow

```go
// à¸—à¸±à¹‰à¸‡ 2 entry points à¹€à¸£à¸µà¸¢à¸à¹ƒà¸Šà¹‰à¹à¸šà¸šà¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™:

// 1. Extract frames (existing logic)
frames := extractFrames(ctx, videoSource, timestamps)

// 2. Classify all frames (NEW)
results, err := classifier.ClassifyBatch(ctx, framesDir)
// results = map[string]ClassificationResult

// 3. Separate safe/nsfw
safeFrames, nsfwFrames := classifier.SeparateByResult(frames, results)

// 4. If not enough safe â†’ extract more (Multi-Round)
if len(safeFrames) < minSafeImages {
    // ... adaptive extraction logic
}

// 5. Upload to separate folders
uploadToS3(safeFrames, "gallery/{code}/safe/")
uploadToS3(nsfwFrames[:30], "gallery/{code}/nsfw/")  // Max 30
```

---

## Technical Implementation

### 1. Database Schema Changes

**Video model (gofiber_starter):**
```go
// à¹€à¸à¸´à¹ˆà¸¡ fields
GallerySafeCount int    `gorm:"default:0"`  // à¸ˆà¸³à¸™à¸§à¸™à¸ à¸²à¸ safe
GalleryNsfwCount int    `gorm:"default:0"`  // à¸ˆà¸³à¸™à¸§à¸™à¸ à¸²à¸ nsfw
GalleryStatus    string `gorm:"size:20;default:'pending'"` // pending|processing|ready
```

### 2. Storage Structure

**Current:**
```
gallery/{videoCode}/
â”œâ”€â”€ 001.jpg
â”œâ”€â”€ 002.jpg
â”œâ”€â”€ ...
â””â”€â”€ 100.jpg
```

**Proposed:**
```
gallery/{videoCode}/
â”œâ”€â”€ safe/
â”‚   â”œâ”€â”€ 001.jpg
â”‚   â”œâ”€â”€ 005.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ nsfw/
â”‚   â”œâ”€â”€ 002.jpg
â”‚   â”œâ”€â”€ 003.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ metadata.json  # classification results
```

### 3. NSFW Classification Service

**Option A: Python Microservice (Recommended)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python_nsfw_classifier/                                 â”‚
â”‚  â”œâ”€â”€ main.py              # FastAPI server               â”‚
â”‚  â”œâ”€â”€ classifier.py        # NudeNet + Falconsai          â”‚
â”‚  â””â”€â”€ requirements.txt                                    â”‚
â”‚                                                          â”‚
â”‚  Endpoints:                                              â”‚
â”‚  - POST /classify         # Classify single image        â”‚
â”‚  - POST /classify-batch   # Classify multiple images     â”‚
â”‚                                                          â”‚
â”‚  Response: { "is_safe": true, "nsfw_score": 0.12 }       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Option B: Embedded in Go Worker**
- Use Python subprocess for each image
- Slower but simpler deployment

### 4. GalleryJob Update

```go
type GalleryJob struct {
    // ... existing fields ...

    // New fields
    ClassifyNSFW    bool `json:"classify_nsfw"`     // Enable NSFW classification
    MinSafeImages   int  `json:"min_safe_images"`   // Minimum safe images required (default 12)
    MaxExtraFrames  int  `json:"max_extra_frames"`  // Max additional frames to try (default 50)
}
```

### 5. Gallery Handler Changes

```go
func (h *GalleryHandler) ProcessJob(ctx context.Context, job *models.GalleryJob) error {
    // 1. Extract frames (existing logic)
    frames, err := h.extractFramesFromHLS(ctx, job, outputDir, progressCallback)

    // 2. NEW: Classify each frame
    safeFrames := []string{}
    nsfwFrames := []string{}

    for _, frame := range frames {
        result, err := h.classifyImage(ctx, frame)
        if result.IsSafe {
            safeFrames = append(safeFrames, frame)
        } else {
            nsfwFrames = append(nsfwFrames, frame)
        }
    }

    // 3. NEW: If not enough safe images, extract more
    if len(safeFrames) < job.MinSafeImages {
        extraFrames := h.extractExtraFrames(ctx, job, len(safeFrames), job.MinSafeImages)
        // ... classify extra frames ...
    }

    // 4. Upload to separate folders
    h.uploadGalleryImages(ctx, safeFrames, job.OutputPath+"/safe", job.VideoCode)
    h.uploadGalleryImages(ctx, nsfwFrames, job.OutputPath+"/nsfw", job.VideoCode)

    // 5. Update DB with counts
    h.updateVideoGallery(ctx, job.VideoID, len(safeFrames), len(nsfwFrames))
}
```

---

## SEO Worker Impact

### Current SEO Worker Flow:
```
1. Get gallery images from S3
2. Download each image
3. Run NSFW classification
4. Filter safe images
5. Try to blur NSFW images (often fails)
6. Use whatever images are available
```

### New SEO Worker Flow:
```
1. Get safe_count from DB
2. If safe_count >= 12:
     â†’ Download only from gallery/{code}/safe/
     â†’ Use directly (no classification needed!)
3. If safe_count < 12:
     â†’ Download safe images
     â†’ Consider using Smart Blur on NSFW images
     â†’ Or just use fewer images
```

---

## Implementation Order

### Sprint 1: Shared Classifier Module (2-3 days)
```
infrastructure/classifier/
â”œâ”€â”€ classify_batch.py    # Python NudeNet
â”œâ”€â”€ nsfw_classifier.go   # Go wrapper (subprocess)
â””â”€â”€ types.go             # Structs
```
- [ ] à¸ªà¸£à¹‰à¸²à¸‡ `classify_batch.py` - NudeNet batch classification
- [ ] à¸ªà¸£à¹‰à¸²à¸‡ `nsfw_classifier.go` - Go wrapper à¹€à¸£à¸µà¸¢à¸ Python subprocess
- [ ] à¸ªà¸£à¹‰à¸²à¸‡ `types.go` - ClassificationResult, ClassificationStats
- [ ] à¸—à¸”à¸ªà¸­à¸šà¸à¸±à¸š sample images

### Sprint 2: Entry Point 1 - Transcode Path (2 days)
```
infrastructure/transcoder/gallery.go  â† à¹à¸à¹‰à¹„à¸‚
```
- [ ] à¹à¸à¹‰ `GenerateGallery()` à¹€à¸à¸´à¹ˆà¸¡ classification step
- [ ] à¹€à¸à¸´à¹ˆà¸¡ Multi-Round extraction logic
- [ ] à¹à¸¢à¸ upload safe/ à¹à¸¥à¸° nsfw/ folders
- [ ] à¸ˆà¸³à¸à¸±à¸” nsfw à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™ 30 à¸ à¸²à¸
- [ ] à¹€à¸à¸´à¹ˆà¸¡ logging stats

### Sprint 3: Entry Point 2 - NATS Job Path (2 days)
```
use_cases/gallery_handler.go  â† à¹à¸à¹‰à¹„à¸‚
```
- [ ] à¹à¸à¹‰ `ProcessJob()` à¹€à¸à¸´à¹ˆà¸¡ classification step
- [ ] à¹€à¸à¸´à¹ˆà¸¡ Multi-Round extraction logic
- [ ] à¹à¸¢à¸ upload safe/ à¹à¸¥à¸° nsfw/ folders
- [ ] à¸ˆà¸³à¸à¸±à¸” nsfw à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™ 30 à¸ à¸²à¸
- [ ] à¹€à¸à¸´à¹ˆà¸¡ logging stats

### Sprint 4: Database & API Updates (1 day)
```
gofiber_starter/
â”œâ”€â”€ domain/models/video.go        # à¹€à¸à¸´à¹ˆà¸¡ fields
â”œâ”€â”€ domain/dto/video.go           # à¹€à¸à¸´à¹ˆà¸¡ DTO
â””â”€â”€ interfaces/api/handlers/      # à¹à¸à¹‰ API response
```
- [ ] à¹€à¸à¸´à¹ˆà¸¡ `GallerySafeCount`, `GalleryNsfwCount` fields
- [ ] à¸­à¸±à¸à¹€à¸”à¸— `UpdateGallery` API à¸£à¸±à¸š counts à¹ƒà¸«à¸¡à¹ˆ
- [ ] Migration script

### Sprint 5: SEO Worker Update (1 day)
```
_seo_worker/
â”œâ”€â”€ infrastructure/imageselector/  # à¹à¸à¹‰à¹„à¸‚
â””â”€â”€ use_cases/seo_handler.go       # à¹à¸à¹‰à¹„à¸‚
```
- [ ] à¹ƒà¸Šà¹‰ pre-classified images à¸ˆà¸²à¸ safe/ folder
- [ ] à¸¥à¸š classification logic à¹€à¸”à¸´à¸¡ (à¹„à¸¡à¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™à¹à¸¥à¹‰à¸§)
- [ ] à¹ƒà¸Šà¹‰ safe_count à¸ˆà¸²à¸ DB

### Sprint 6: Backfill Existing Videos (Optional)
- [ ] à¸ªà¸£à¹‰à¸²à¸‡ batch job à¸ªà¸³à¸«à¸£à¸±à¸š video à¹€à¸à¹ˆà¸²
- [ ] Classify à¹à¸¥à¸° re-organize folders
- [ ] Update DB counts

---

## Configuration

```yaml
# config.yaml
gallery:
  enabled: true
  image_count: 100
  min_safe_images: 12
  max_extra_frames: 50

nsfw_classifier:
  service_url: "http://localhost:8000"
  timeout: 30s

  # Classification thresholds
  nsfw_threshold: 0.3     # Score above this = NSFW

  # Models to use
  use_nudenet: true       # Fast, region-based
  use_falconsai: false    # Slow, whole-image (use for verification only)
```

---

## Estimated Impact

| Metric | Before | After |
|--------|--------|-------|
| Safe images per video | ~6 | 12+ |
| SEO Worker classification time | 30-60s | 0s (pre-classified) |
| Gallery generation time | 2-3 min | 4-5 min (+classification) |
| Storage overhead | 1x | ~1.1x (metadata.json) |

---

## Design Decisions (Confirmed)

### 1. Folder Structure: à¹ƒà¸Šà¹‰ `safe/` à¹à¸¥à¸° `nsfw/` folders

**Decision:** à¹ƒà¸Šà¹‰ folder à¹à¸¢à¸ à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰ suffix

**à¹€à¸«à¸•à¸¸à¸œà¸¥:**
- à¸—à¸³ ACL (Access Control) à¹„à¸”à¹‰à¸‡à¹ˆà¸²à¸¢
- `safe/` â†’ Public access
- `nsfw/` â†’ à¸•à¹‰à¸­à¸‡à¸¡à¸µ Signed URL à¸«à¸£à¸·à¸­ Membership
- à¸‡à¹ˆà¸²à¸¢à¸•à¹ˆà¸­à¸à¸²à¸£ list files, backup, migrate

```
gallery/{videoCode}/
â”œâ”€â”€ safe/           â† Public CDN
â”‚   â”œâ”€â”€ 001.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ nsfw/           â† Signed URL only
â”‚   â”œâ”€â”€ 002.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ metadata.json
```

### 2. NSFW Classification Strategy: Two-Tier System

**Decision:** NudeNet (Primary) + Falconsai (Cover Gatekeeper)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Classification Flow                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  All Images â”€â”€â–º NudeNet (Fast) â”€â”€â–º safe/ or nsfw/               â”‚
â”‚                                                                  â”‚
â”‚  Cover Candidate â”€â”€â–º Falconsai (Slow) â”€â”€â–º Final Cover           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**à¹€à¸«à¸•à¸¸à¸œà¸¥:**
- **NudeNet**: à¹€à¸£à¹‡à¸§, à¸£à¸°à¸šà¸¸à¸ˆà¸¸à¸”à¹„à¸”à¹‰à¸”à¸µ (bounding boxes) - à¹ƒà¸Šà¹‰à¸à¸±à¸šà¸—à¸¸à¸à¸ à¸²à¸
- **Falconsai**: à¸¡à¸­à¸‡à¸šà¸£à¸´à¸šà¸—à¹€à¸à¹ˆà¸‡à¸à¸§à¹ˆà¸² à¹à¸•à¹ˆà¸Šà¹‰à¸² - à¹ƒà¸Šà¹‰à¹€à¸‰à¸à¸²à¸° Cover à¸—à¸µà¹ˆà¸ˆà¸°à¹‚à¸Šà¸§à¹Œà¸šà¸™ Google
- à¸¥à¸”à¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¹‚à¸”à¸™à¹à¸šà¸™à¹„à¸”à¹‰ 100% à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸—à¸³à¹ƒà¸«à¹‰à¸£à¸°à¸šà¸šà¸Šà¹‰à¸²à¹€à¸à¸´à¸™à¹„à¸›

```go
// Classification logic
func (h *GalleryHandler) classifyImage(ctx context.Context, imagePath string) (*ClassificationResult, error) {
    // Step 1: NudeNet (always)
    nudenetResult := h.nudenetClassify(imagePath)

    // For gallery images: NudeNet is enough
    return nudenetResult
}

func (h *GalleryHandler) selectCoverImage(ctx context.Context, safeImages []string) (string, error) {
    // Step 1: Sort by face_score, aesthetic_score
    candidates := h.rankBestCandidates(safeImages, 3) // Top 3

    // Step 2: Verify with Falconsai (strict check for Google)
    for _, candidate := range candidates {
        falconsaiResult := h.falconsaiClassify(candidate)
        if falconsaiResult.Score < 0.2 { // Very strict for cover
            return candidate, nil
        }
    }

    // Fallback: use safest candidate
    return candidates[0], nil
}
```

### 3. Failed Classification: Safety First

**Decision:** à¸–à¹‰à¸² classifier à¸¥à¹‰à¸¡à¹€à¸«à¸¥à¸§ â†’ à¸–à¸·à¸­à¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ **NSFW**

**à¹€à¸«à¸•à¸¸à¸œà¸¥:**
- à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸ à¸²à¸à¸«à¸¥à¸¸à¸”à¹„à¸›à¸—à¸µà¹ˆà¸«à¸™à¹‰à¸² Public
- Better safe than sorry
- à¸ªà¸²à¸¡à¸²à¸£à¸– re-classify à¸—à¸µà¸«à¸¥à¸±à¸‡à¹„à¸”à¹‰

```go
func (h *GalleryHandler) classifyImage(ctx context.Context, imagePath string) (*ClassificationResult, error) {
    result, err := h.nudenetClassify(imagePath)
    if err != nil {
        // Classification failed â†’ treat as NSFW
        h.logger.Warn("classification failed, treating as NSFW",
            "image", imagePath,
            "error", err,
        )
        return &ClassificationResult{
            IsSafe:    false,
            NsfwScore: 1.0,
            Error:     err.Error(),
        }, nil
    }
    return result, nil
}
```

### 4. Re-run Existing Videos: Optional Backfill

**Decision:** à¹„à¸¡à¹ˆà¸šà¸±à¸‡à¸„à¸±à¸š backfill à¸—à¸±à¸™à¸—à¸µ à¹à¸•à¹ˆà¸¡à¸µ option

**à¹à¸™à¸§à¸—à¸²à¸‡:**
- Video à¹ƒà¸«à¸¡à¹ˆ â†’ à¹ƒà¸Šà¹‰à¸£à¸°à¸šà¸šà¹ƒà¸«à¸¡à¹ˆà¹€à¸¥à¸¢
- Video à¹€à¸à¹ˆà¸² â†’ à¸—à¸³ lazy classification à¹€à¸¡à¸·à¹ˆà¸­ SEO Worker à¸•à¹‰à¸­à¸‡à¸à¸²à¸£
- Admin option â†’ Manual trigger backfill à¸ªà¸³à¸«à¸£à¸±à¸š video à¸ªà¸³à¸„à¸±à¸

---

## Additional Design Decisions

### 5. NSFW Storage Limit: à¹€à¸à¹‡à¸šà¹„à¸¡à¹ˆà¹€à¸à¸´à¸™ 30 à¸ à¸²à¸

**Decision:** à¹€à¸à¹‡à¸š `nsfw/` folder à¹„à¸§à¹‰ à¹à¸•à¹ˆà¸ˆà¸³à¸à¸±à¸”à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™ 30 à¸ à¸²à¸

**à¹€à¸«à¸•à¸¸à¸œà¸¥:**
- à¸›à¸£à¸°à¸«à¸¢à¸±à¸” storage (à¹„à¸¡à¹ˆà¹€à¸à¹‡à¸š 70-90 à¸ à¸²à¸ NSFW à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”)
- à¸¡à¸µà¹„à¸§à¹‰à¹ƒà¸Šà¹‰à¸ªà¸³à¸«à¸£à¸±à¸š Members (Signed URL)
- à¹€à¸¥à¸·à¸­à¸à¹€à¸à¹‡à¸šà¸ à¸²à¸à¸—à¸µà¹ˆà¸¡à¸µ quality à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸” (face_score, aesthetic_score)

```go
const MaxNsfwImages = 30

func (h *GalleryHandler) selectBestNsfwImages(nsfwImages []ClassifiedImage) []ClassifiedImage {
    if len(nsfwImages) <= MaxNsfwImages {
        return nsfwImages
    }

    // Sort by quality score (face + aesthetic)
    sort.Slice(nsfwImages, func(i, j int) bool {
        scoreI := nsfwImages[i].FaceScore + nsfwImages[i].AestheticScore
        scoreJ := nsfwImages[j].FaceScore + nsfwImages[j].AestheticScore
        return scoreI > scoreJ
    })

    // Keep top 30
    return nsfwImages[:MaxNsfwImages]
}
```

**Storage Structure:**
```
gallery/{videoCode}/
â”œâ”€â”€ safe/           â† à¸—à¸¸à¸à¸ à¸²à¸ safe (12-20 à¸ à¸²à¸)
â”œâ”€â”€ nsfw/           â† à¹€à¸‰à¸à¸²à¸° top 30 à¸ à¸²à¸ (à¹€à¸£à¸µà¸¢à¸‡à¸•à¸²à¸¡ quality)
â””â”€â”€ metadata.json
```

### 6. Analytics: Simple Logging

**Decision:** à¹€à¸à¹‡à¸š Simple Logging à¸ªà¸³à¸«à¸£à¸±à¸š tuning threshold à¹ƒà¸™à¸­à¸™à¸²à¸„à¸•

**à¹€à¸«à¸•à¸¸à¸œà¸¥:**
- Track False Positive/Negative à¹€à¸à¸·à¹ˆà¸­à¸›à¸£à¸±à¸š `nsfw_threshold`
- à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸‹à¸±à¸šà¸‹à¹‰à¸­à¸™ à¹à¸„à¹ˆ log à¸•à¸±à¸§à¹€à¸¥à¸‚à¸à¸·à¹‰à¸™à¸à¸²à¸™

```go
type ClassificationStats struct {
    VideoCode       string    `json:"video_code"`
    TotalFrames     int       `json:"total_frames"`
    SafeCount       int       `json:"safe_count"`
    NsfwCount       int       `json:"nsfw_count"`
    RoundsUsed      int       `json:"rounds_used"`
    AvgNsfwScore    float64   `json:"avg_nsfw_score"`
    ProcessingTime  float64   `json:"processing_time_sec"`
    Timestamp       time.Time `json:"timestamp"`
}

// Log to file for future analysis
func (h *GalleryHandler) logClassificationStats(stats ClassificationStats) {
    h.logger.Info("classification_stats",
        "video_code", stats.VideoCode,
        "total", stats.TotalFrames,
        "safe", stats.SafeCount,
        "nsfw", stats.NsfwCount,
        "rounds", stats.RoundsUsed,
        "avg_score", stats.AvgNsfwScore,
        "time_sec", stats.ProcessingTime,
    )
}
```

**Log Output Example:**
```json
{
  "level": "INFO",
  "msg": "classification_stats",
  "video_code": "ABC123",
  "total": 165,
  "safe": 15,
  "nsfw": 150,
  "rounds": 4,
  "avg_score": 0.72,
  "time_sec": 45.2
}
```

**Future Use:**
- à¸–à¹‰à¸² `avg_nsfw_score` à¸•à¹ˆà¸³à¹à¸•à¹ˆà¸¢à¸±à¸‡à¸–à¸¹à¸ classify à¹€à¸›à¹‡à¸™ nsfw â†’ à¸¥à¸” threshold
- à¸–à¹‰à¸²à¸¡à¸µ report à¸§à¹ˆà¸²à¸ à¸²à¸ safe à¸«à¸¥à¸¸à¸”à¹„à¸› â†’ à¹€à¸à¸´à¹ˆà¸¡ threshold

---

## Summary: All Decisions Confirmed

| # | Topic | Decision |
|---|-------|----------|
| 1 | Folder Structure | à¹ƒà¸Šà¹‰ `safe/` à¹à¸¥à¸° `nsfw/` folders |
| 2 | Classification | NudeNet (all) + Falconsai (cover only) |
| 3 | Failed Classification | à¸–à¸·à¸­à¸§à¹ˆà¸² NSFW (Safety First) |
| 4 | Backfill | Optional, lazy classification |
| 5 | NSFW Storage | à¹€à¸à¹‡à¸šà¹„à¸¡à¹ˆà¹€à¸à¸´à¸™ 30 à¸ à¸²à¸ (top quality) |
| 6 | Analytics | Simple Logging à¸ªà¸³à¸«à¸£à¸±à¸š tuning |

**Status: Ready for Implementation**

---

*Created: 2026-02-23*
*Updated: 2026-02-23 - Added design decisions based on review*
*Updated: 2026-02-24 - Added Three-Tier Super Safe system for Google SafeSearch compliance*

---

## ğŸ“Š Implementation Status

### âœ… IMPLEMENTED (Phase 1-2 + Phase 3 Complete)

| Feature | File | Status |
|---------|------|--------|
| Two-Tier Classification (safe/nsfw) | `types.go` | âœ… Done |
| Dual Model (Falconsai + NudeNet) | `classify_batch.py` | âœ… Done |
| Threshold 0.3 | `DefaultConfig()` | âœ… Done |
| Multi-Round Extraction (5 rounds) | `gallery_handler.go` | âœ… Done |
| MinSafeImages: 12, MaxNsfwImages: 30 | Config | âœ… Done |
| Quality sorting (face + aesthetic) | `nsfw_classifier.go` | âœ… Done |
| Safety First (errors â†’ nsfw) | `gallery_handler.go` | âœ… Done |
| ProcessJobWithClassification() | `gallery_handler.go` | âœ… Done |
| GenerateGalleryWithClassification() | `gallery_classified.go` | âœ… Done |
| **Three-Tier (super_safe/)** | `gallery_classified.go`, `gallery_handler.go` | âœ… Done |
| **SuperSafeThreshold 0.15** | `types.go`, `classify_batch.py` | âœ… Done |
| **IsSuperSafe field** | `types.go`, `classify_batch.py` | âœ… Done |
| **MinSuperSafeImages: 10** | `gallery_classified.go` | âœ… Done |
| **Face requirement for super_safe** | `classify_batch.py`, `nsfw_classifier.go` | âœ… Done |
| Upload to /super_safe/, /safe/, /nsfw/ | `gallery_handler.go`, `gallery_classified.go` | âœ… Done |

### â³ PENDING

| Feature | Description | Priority |
|---------|-------------|----------|
| **FalconsaiScore/NudenetScore** | à¹à¸¢à¸à¹€à¸à¹‡à¸š score à¹à¸•à¹ˆà¸¥à¸° model à¹ƒà¸™ result | LOW |
| **metadata.json** | à¸šà¸±à¸™à¸—à¸¶à¸ classification results | LOW |

### Current Storage Structure (Three-Tier - Implemented)

```
gallery/{videoCode}/
â”œâ”€â”€ super_safe/         â† score < 0.15 + face (Public SEO)
â”‚   â”œâ”€â”€ 001.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ safe/               â† score 0.15-0.3 (Lazy load)
â”‚   â”œâ”€â”€ 005.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ nsfw/               â† score >= 0.3 (Member only)
â”‚   â”œâ”€â”€ 010.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ (metadata.json - not yet)
```

---

---

## ğŸ†• [PROPOSED] Phase 3: Three-Tier Image Safety System (Google SafeSearch Compliance)

> **à¸›à¸±à¸à¸«à¸²à¹ƒà¸«à¸¡à¹ˆ:** Google Cloud Vision à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸”à¸¹à¹à¸„à¹ˆ explicit content
> à¹à¸•à¹ˆà¸”à¸¹ **Suggestive Content** (à¸ à¸²à¸à¸Šà¸µà¹‰à¸™à¸³, à¸ªà¸µà¸«à¸™à¹‰à¸²à¹€à¸„à¸¥à¸´à¹‰à¸¡, à¸—à¹ˆà¸²à¸—à¸²à¸‡à¹ƒà¸à¸¥à¹‰à¸Šà¸´à¸”) à¸”à¹‰à¸§à¸¢

### Current System (Two-Tier)

```
nsfw_score < 0.3  â†’  /safe/   (Public + Member)
nsfw_score >= 0.3 â†’  /nsfw/   (Member only)
```

**à¸›à¸±à¸à¸«à¸²:** à¸ à¸²à¸à¸—à¸µà¹ˆ score 0.2 à¸­à¸²à¸ˆà¸–à¸¹à¸ Google à¸•à¸µà¸§à¹ˆà¸² "Racy" (suggestive)

### Proposed System (Three-Tier)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  THREE-TIER CLASSIFICATION                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  SUPER_SAFE = nsfw_score < 0.15 AND face_score > 0.1            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸«à¸™à¹‰à¸²à¸„à¸™à¹ƒà¸™à¸ à¸²à¸ (à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆà¸ à¸²à¸à¸«à¹‰à¸­à¸‡à¹€à¸›à¸¥à¹ˆà¸²/à¸‰à¸²à¸à¸«à¸¥à¸±à¸‡)              â”‚
â”‚  â€¢ à¹ƒà¸Šà¹‰à¹€à¸›à¹‡à¸™ Thumbnail/OG Image                                   â”‚
â”‚  â€¢ Google Bot à¹€à¸«à¹‡à¸™à¹„à¸”à¹‰                                           â”‚
â”‚                                                                  â”‚
â”‚  SAFE = nsfw_score < 0.3 AND NOT SUPER_SAFE                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ à¸ à¸²à¸à¸—à¸µà¹ˆ score 0.15-0.3 à¸«à¸£à¸·à¸­ à¹„à¸¡à¹ˆà¸¡à¸µà¸«à¸™à¹‰à¸²à¸„à¸™                       â”‚
â”‚  â€¢ à¸‹à¹ˆà¸­à¸™à¸«à¸¥à¸±à¸‡à¸›à¸¸à¹ˆà¸¡ "à¸”à¸¹à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡"                                   â”‚
â”‚  â€¢ Google Bot à¹„à¸¡à¹ˆà¹€à¸«à¹‡à¸™ URL                                       â”‚
â”‚                                                                  â”‚
â”‚  NSFW = nsfw_score >= 0.3                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ à¸ à¸²à¸à¸—à¸µà¹ˆ explicit                                              â”‚
â”‚  â€¢ Signed URL only (Member)                                     â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš ï¸ à¸ªà¸³à¸„à¸±à¸: Super Safe à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸«à¸™à¹‰à¸²à¸„à¸™

```
à¸›à¸±à¸à¸«à¸²à¸—à¸µà¹ˆà¹€à¸„à¸¢à¹€à¸ˆà¸­:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âŒ à¸ à¸²à¸à¸«à¹‰à¸­à¸‡à¹€à¸›à¸¥à¹ˆà¸² (nsfw_score: 0.02, face_score: 0.0)
   â†’ à¸œà¹ˆà¸²à¸™ threshold 0.15 à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¸¡à¸µà¸„à¸™ = à¹„à¸¡à¹ˆà¸„à¸§à¸£à¹€à¸›à¹‡à¸™ super_safe

âŒ à¸ à¸²à¸à¸‰à¸²à¸à¸«à¸¥à¸±à¸‡/à¹€à¸Ÿà¸­à¸£à¹Œà¸™à¸´à¹€à¸ˆà¸­à¸£à¹Œ (nsfw_score: 0.05, face_score: 0.0)
   â†’ à¸œà¹ˆà¸²à¸™ threshold 0.15 à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¸¡à¸µà¸„à¸™ = à¹„à¸¡à¹ˆà¸„à¸§à¸£à¹€à¸›à¹‡à¸™ super_safe

âœ… à¸ à¸²à¸à¸„à¸™à¸¢à¸·à¸™à¸„à¸¸à¸¢ (nsfw_score: 0.08, face_score: 0.35)
   â†’ à¸œà¹ˆà¸²à¸™à¸—à¸±à¹‰à¸‡ 2 à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚ = super_safe âœ“

âœ… à¸ à¸²à¸à¸«à¸™à¹‰à¸²à¸™à¸±à¸à¹à¸ªà¸”à¸‡ (nsfw_score: 0.12, face_score: 0.45)
   â†’ à¸œà¹ˆà¸²à¸™à¸—à¸±à¹‰à¸‡ 2 à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚ = super_safe âœ“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Face Score Logic (à¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§à¹ƒà¸™ classify_batch.py)

```python
def _calculate_face_score(self, img: np.ndarray) -> float:
    """
    à¹ƒà¸Šà¹‰ Haar Cascade à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¸«à¸™à¹‰à¸²

    Returns:
        0.0  = à¹„à¸¡à¹ˆà¹€à¸ˆà¸­à¸«à¸™à¹‰à¸²à¹€à¸¥à¸¢ (à¸ à¸²à¸à¸«à¹‰à¸­à¸‡/à¸‰à¸²à¸à¸«à¸¥à¸±à¸‡)
        0.1+ = à¹€à¸ˆà¸­à¸«à¸™à¹‰à¸²à¹€à¸¥à¹‡à¸à¹† (à¸«à¸™à¹‰à¸² < 1% à¸‚à¸­à¸‡à¸ à¸²à¸)
        0.3+ = à¹€à¸ˆà¸­à¸«à¸™à¹‰à¸²à¸‚à¸™à¸²à¸”à¸à¸¥à¸²à¸‡ (à¸«à¸™à¹‰à¸² 5-10% à¸‚à¸­à¸‡à¸ à¸²à¸)
        0.5+ = à¹€à¸ˆà¸­à¸«à¸™à¹‰à¸²à¹ƒà¸«à¸à¹ˆ (à¸«à¸™à¹‰à¸² > 10% à¸‚à¸­à¸‡à¸ à¸²à¸)
    """
    faces = self.face_cascade.detectMultiScale(gray, ...)

    if len(faces) == 0:
        return 0.0  # à¹„à¸¡à¹ˆà¸¡à¸µà¸«à¸™à¹‰à¸²à¸„à¸™

    # à¸„à¸³à¸™à¸§à¸“ ratio à¸‚à¸­à¸‡à¸«à¸™à¹‰à¸²à¸•à¹ˆà¸­à¸ à¸²à¸à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
    face_ratio = face_area / img_area
    return min(1.0, face_ratio * 5)
```

### Super Safe Selection Criteria

```python
# à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚à¸ªà¸³à¸«à¸£à¸±à¸š SUPER_SAFE
def is_super_safe(result):
    return (
        result.nsfw_score < 0.15 and      # à¸•à¹‰à¸­à¸‡ safe à¸¡à¸²à¸à¹†
        result.face_score > 0.1 and        # à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸«à¸™à¹‰à¸²à¸„à¸™
        result.error == ""                 # à¹„à¸¡à¹ˆà¸¡à¸µ error
    )

# à¸ à¸²à¸à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸œà¹ˆà¸²à¸™ super_safe à¹à¸•à¹ˆà¸¢à¸±à¸‡ safe
def is_safe_only(result):
    return (
        result.nsfw_score < 0.30 and      # safe threshold
        not is_super_safe(result)          # à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¸œà¹ˆà¸²à¸™ super_safe
    )
```

### Storage Structure Update

```
gallery/{videoCode}/
â”œâ”€â”€ super_safe/         â† NEW: à¸ªà¸³à¸«à¸£à¸±à¸š Public SEO (score < 0.15)
â”‚   â”œâ”€â”€ 001.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ safe/               â† Borderline (score 0.15-0.3)
â”‚   â”œâ”€â”€ 005.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ nsfw/               â† Member only (score >= 0.3)
â”‚   â”œâ”€â”€ 010.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ metadata.json
```

### Implementation Changes

#### 1. Update Classifier Constants

```go
// infrastructure/classifier/types.go

const (
    // Three-Tier thresholds
    SuperSafeThreshold = 0.15  // à¸ªà¸³à¸«à¸£à¸±à¸š Public Featured
    SafeThreshold      = 0.30  // à¸ªà¸³à¸«à¸£à¸±à¸š Member

    // Minimum requirements
    MinSuperSafeImages = 10    // à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸­à¸¢à¹ˆà¸²à¸‡à¸™à¹‰à¸­à¸¢ 10 à¸ à¸²à¸ super safe
    MinSafeImages      = 12    // à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸­à¸¢à¹ˆà¸²à¸‡à¸™à¹‰à¸­à¸¢ 12 à¸ à¸²à¸ safe
    MaxNsfwImages      = 30    // à¹€à¸à¹‡à¸šà¹„à¸¡à¹ˆà¹€à¸à¸´à¸™ 30 à¸ à¸²à¸ nsfw
)
```

#### 2. Update ClassificationResult & Config

```go
// infrastructure/classifier/types.go

// ClassifierConfig - à¹€à¸à¸´à¹ˆà¸¡ MinSuperSafeImages
type ClassifierConfig struct {
    PythonPath         string
    ScriptPath         string
    NsfwThreshold      float64 // 0.3
    SuperSafeThreshold float64 // NEW: 0.15
    MinFaceScore       float64 // NEW: 0.1
    Timeout            int
    MaxNsfwImages      int     // 30
    MinSafeImages      int     // 12
    MinSuperSafeImages int     // NEW: 10 (à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸­à¸¢à¹ˆà¸²à¸‡à¸™à¹‰à¸­à¸¢ 10 à¸ à¸²à¸ super_safe)
}

func DefaultConfig() ClassifierConfig {
    return ClassifierConfig{
        PythonPath:         "python",
        ScriptPath:         "infrastructure/classifier/classify_batch.py",
        NsfwThreshold:      0.3,
        SuperSafeThreshold: 0.15,  // NEW
        MinFaceScore:       0.1,   // NEW
        Timeout:            90,
        MaxNsfwImages:      30,
        MinSafeImages:      12,
        MinSuperSafeImages: 10,    // NEW
    }
}

type ClassificationResult struct {
    Filename       string  `json:"filename"`
    IsSuperSafe    bool    `json:"is_super_safe"`   // NEW: < 0.15 + face
    IsSafe         bool    `json:"is_safe"`         // < 0.30
    NsfwScore      float64 `json:"nsfw_score"`
    FalconsaiScore float64 `json:"falconsai_score"` // NEW: for tracking
    NudenetScore   float64 `json:"nudenet_score"`   // NEW: for tracking
    FaceScore      float64 `json:"face_score"`
    AestheticScore float64 `json:"aesthetic_score"`
    Error          string  `json:"error,omitempty"`
}

// SeparatedImages à¸­à¸±à¸à¹€à¸”à¸—à¹€à¸›à¹‡à¸™ 3 à¸£à¸°à¸”à¸±à¸š
type SeparatedImages struct {
    SuperSafe []ClassificationResult `json:"super_safe"` // < 0.15 + face
    Safe      []ClassificationResult `json:"safe"`       // 0.15-0.3 or no face
    Nsfw      []ClassificationResult `json:"nsfw"`       // >= 0.3
    Error     []ClassificationResult `json:"error"`
}
```

#### 3. Update Python Classifier

```python
# infrastructure/classifier/classify_batch.py

SUPER_SAFE_THRESHOLD = 0.15  # NEW
SAFE_THRESHOLD = 0.30
MIN_FACE_SCORE = 0.1  # NEW: à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸«à¸™à¹‰à¸²à¸„à¸™à¹ƒà¸™à¸ à¸²à¸

def classify(self, image_path: str) -> Dict[str, Any]:
    # ... existing classification logic ...

    nsfw_score = max(falconsai_score, nudenet_score)
    face_score = self._calculate_face_score(cv_image)  # à¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§

    # Three-tier classification
    # âš ï¸ SUPER_SAFE à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸«à¸™à¹‰à¸²à¸„à¸™ (face_score > 0.1)
    # à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸ à¸²à¸à¸«à¹‰à¸­à¸‡à¹€à¸›à¸¥à¹ˆà¸²/à¸‰à¸²à¸à¸«à¸¥à¸±à¸‡à¸«à¸¥à¸¸à¸”à¹„à¸›à¹€à¸›à¹‡à¸™ featured image
    is_super_safe = (
        nsfw_score < SUPER_SAFE_THRESHOLD and
        face_score > MIN_FACE_SCORE  # à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸«à¸™à¹‰à¸²à¸„à¸™!
    )
    is_safe = nsfw_score < SAFE_THRESHOLD

    return {
        "filename": filename,
        "is_super_safe": is_super_safe,  # NEW: à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸«à¸™à¹‰à¸²à¸„à¸™
        "is_safe": is_safe,
        "nsfw_score": round(nsfw_score, 4),
        "falconsai_score": round(falconsai_score, 4),
        "nudenet_score": round(nudenet_score, 4),
        "face_score": round(face_score, 4),  # à¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§
        "aesthetic_score": round(aesthetic_score, 4),
        "error": ""
    }
```

**à¸«à¸¡à¸²à¸¢à¹€à¸«à¸•à¸¸:** `face_score` à¸¡à¸µà¸à¸²à¸£à¸„à¸³à¸™à¸§à¸“à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§à¹ƒà¸™ `_calculate_face_score()` à¹ƒà¸Šà¹‰ Haar Cascade à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¸«à¸™à¹‰à¸²

#### 4. Update SeparateResults

```go
// infrastructure/classifier/nsfw_classifier.go

const (
    SuperSafeThreshold = 0.15
    SafeThreshold      = 0.30
    MinFaceScore       = 0.1  // à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸«à¸™à¹‰à¸²à¸„à¸™à¹ƒà¸™à¸ à¸²à¸
)

func (c *NSFWClassifier) SeparateResults(results map[string]ClassificationResult) *SeparatedImages {
    separated := &SeparatedImages{
        SuperSafe: make([]ClassificationResult, 0),
        Safe:      make([]ClassificationResult, 0),
        Nsfw:      make([]ClassificationResult, 0),
        Error:     make([]ClassificationResult, 0),
    }

    for _, result := range results {
        if result.Error != "" {
            // Error â†’ treat as NSFW (safety first)
            separated.Error = append(separated.Error, result)

        } else if result.NsfwScore < SuperSafeThreshold && result.FaceScore > MinFaceScore {
            // âš ï¸ SUPER SAFE: à¸•à¹‰à¸­à¸‡à¸œà¹ˆà¸²à¸™à¸—à¸±à¹‰à¸‡ 2 à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚
            // 1. nsfw_score < 0.15 (à¹„à¸¡à¹ˆà¸¡à¸µà¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸­à¸±à¸™à¸•à¸£à¸²à¸¢)
            // 2. face_score > 0.1 (à¸¡à¸µà¸«à¸™à¹‰à¸²à¸„à¸™à¹ƒà¸™à¸ à¸²à¸)
            // à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸ à¸²à¸à¸«à¹‰à¸­à¸‡à¹€à¸›à¸¥à¹ˆà¸²/à¸‰à¸²à¸à¸«à¸¥à¸±à¸‡à¸«à¸¥à¸¸à¸”à¹„à¸›à¹€à¸›à¹‡à¸™ featured image
            separated.SuperSafe = append(separated.SuperSafe, result)

        } else if result.NsfwScore < SafeThreshold {
            // SAFE: à¹„à¸¡à¹ˆà¸œà¹ˆà¸²à¸™ super_safe à¹à¸•à¹ˆà¸¢à¸±à¸‡ safe
            // - à¸­à¸²à¸ˆà¹€à¸›à¹‡à¸™à¸ à¸²à¸à¸«à¹‰à¸­à¸‡ (no face)
            // - à¸«à¸£à¸·à¸­ score 0.15-0.3 (borderline)
            separated.Safe = append(separated.Safe, result)

        } else {
            // NSFW: >= 0.3
            separated.Nsfw = append(separated.Nsfw, result)
        }
    }

    return separated
}
```

**Logic à¸ªà¸³à¸„à¸±à¸:**
```
à¸ à¸²à¸à¸«à¹‰à¸­à¸‡à¹€à¸›à¸¥à¹ˆà¸² (nsfw: 0.05, face: 0.0) â†’ SAFE (à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆ super_safe)
à¸ à¸²à¸à¸„à¸™à¸„à¸¸à¸¢à¸à¸±à¸™ (nsfw: 0.08, face: 0.35) â†’ SUPER_SAFE âœ“
à¸ à¸²à¸ borderline (nsfw: 0.20, face: 0.40) â†’ SAFE (nsfw > 0.15)
```

#### 5. Update Multi-Round Extraction (à¸«à¸²à¸ˆà¸™à¸à¸§à¹ˆà¸²à¸ˆà¸°à¸„à¸£à¸š)

```go
// use_cases/gallery_handler.go

// ProcessJobWithClassification - à¸­à¸±à¸à¹€à¸”à¸—à¹ƒà¸«à¹‰à¸«à¸² super_safe à¸ˆà¸™à¸„à¸£à¸š
func (h *GalleryHandler) ProcessJobWithClassification(ctx context.Context, job *models.GalleryJob) error {
    // ... existing setup ...

    // Track à¸—à¸±à¹‰à¸‡ 3 à¸£à¸°à¸”à¸±à¸š
    var allSuperSafeResults []classifier.ClassificationResult  // NEW
    var allSafeResults []classifier.ClassificationResult
    var allNsfwResults []classifier.ClassificationResult

    for _, round := range extractionRounds {
        // âš ï¸ à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚à¸«à¸¢à¸¸à¸”à¹ƒà¸«à¸¡à¹ˆ: à¸•à¹‰à¸­à¸‡à¹„à¸”à¹‰à¸—à¸±à¹‰à¸‡ super_safe à¹à¸¥à¸° safe
        hasEnoughSuperSafe := len(allSuperSafeResults) >= classifierConfig.MinSuperSafeImages  // >= 10
        hasEnoughSafe := len(allSafeResults) + len(allSuperSafeResults) >= classifierConfig.MinSafeImages  // >= 12

        if hasEnoughSuperSafe && hasEnoughSafe {
            break  // à¸„à¸£à¸šà¹à¸¥à¹‰à¸§ à¸«à¸¢à¸¸à¸”à¹„à¸”à¹‰
        }

        h.logger.Info("extraction round",
            "round", round.name,
            "current_super_safe", len(allSuperSafeResults),
            "current_safe", len(allSafeResults),
            "target_super_safe", classifierConfig.MinSuperSafeImages,
            "target_safe", classifierConfig.MinSafeImages,
        )

        // Extract frames for this round
        frameCount := h.extractRoundFramesFromHLS(...)

        // Classify all frames
        result, _ := nsfwClassifier.ClassifyBatch(ctx, allFramesDir)

        // Separate into 3 tiers
        separated := nsfwClassifier.SeparateResults(result.Results)

        // Move files to appropriate directories
        h.moveClassifiedFilesThreeTier(allFramesDir, superSafeDir, safeDir, nsfwDir, separated)

        // Accumulate results
        allSuperSafeResults = append(allSuperSafeResults, separated.SuperSafe...)
        allSafeResults = append(allSafeResults, separated.Safe...)
        allNsfwResults = append(allNsfwResults, separated.Nsfw...)

        h.logger.Info("round complete",
            "round", round.name,
            "super_safe_found", len(separated.SuperSafe),
            "safe_found", len(separated.Safe),
            "total_super_safe", len(allSuperSafeResults),
            "total_safe", len(allSafeResults),
        )
    }

    // Log final stats
    h.logger.Info("extraction complete",
        "total_super_safe", len(allSuperSafeResults),
        "total_safe", len(allSafeResults),
        "total_nsfw", len(allNsfwResults),
        "rounds_used", roundsUsed,
        "super_safe_target_met", len(allSuperSafeResults) >= classifierConfig.MinSuperSafeImages,
    )

    // ... upload and continue ...
}
```

#### 6. Target Counts

| Tier | Minimum Target | à¸«à¸¢à¸¸à¸”à¹€à¸¡à¸·à¹ˆà¸­ | à¹€à¸«à¸•à¸¸à¸œà¸¥ |
|------|---------------|-----------|--------|
| **super_safe** | 10 à¸ à¸²à¸ | à¹„à¸”à¹‰à¸„à¸£à¸š 10 | à¸ªà¸³à¸«à¸£à¸±à¸š Public SEO (featured, gallery) |
| **safe** (à¸£à¸§à¸¡ super_safe) | 12 à¸ à¸²à¸ | à¹„à¸”à¹‰à¸„à¸£à¸š 12 | Backward compatible |
| **nsfw** | à¹„à¸¡à¹ˆà¸ˆà¸³à¸à¸±à¸” (à¹€à¸à¹‡à¸š max 30) | - | Member only |

**Logic:**
```go
// à¸«à¸¢à¸¸à¸” extraction à¹€à¸¡à¸·à¹ˆà¸­:
// 1. super_safe >= 10 à¸ à¸²à¸ (à¸¡à¸µà¸ à¸²à¸à¸„à¸™à¸ªà¸³à¸«à¸£à¸±à¸š Public)
// 2. total_safe (super_safe + safe) >= 12 à¸ à¸²à¸ (backward compatible)

stopCondition := len(superSafe) >= 10 && (len(superSafe) + len(safe)) >= 12
```

#### 7. Handling Edge Cases

```go
// à¸à¸£à¸“à¸µ video à¹€à¸›à¹‡à¸™ NSFW à¹€à¸à¸·à¸­à¸šà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” (à¹„à¸¡à¹ˆà¸¡à¸µ super_safe à¸à¸­)

if len(allSuperSafeResults) < classifierConfig.MinSuperSafeImages {
    // Option A: à¹ƒà¸Šà¹‰à¸ à¸²à¸à¸—à¸µà¹ˆà¸¡à¸µ face à¸ªà¸¹à¸‡à¸ªà¸¸à¸”à¸ˆà¸²à¸ safe (à¹à¸¡à¹‰ score > 0.15)
    // à¹€à¸¥à¸·à¸­à¸à¸ à¸²à¸à¸—à¸µà¹ˆ "safe à¸—à¸µà¹ˆà¸ªà¸¸à¸”" à¸ˆà¸²à¸à¸—à¸µà¹ˆà¸¡à¸µ
    fallbackImages := selectBestFallbackImages(allSafeResults, classifierConfig.MinSuperSafeImages - len(allSuperSafeResults))
    allSuperSafeResults = append(allSuperSafeResults, fallbackImages...)

    // Option B: Flag video à¸§à¹ˆà¸² "limited_public_gallery"
    h.logger.Warn("not enough super_safe images",
        "video_code", job.VideoCode,
        "super_safe_count", len(allSuperSafeResults),
        "target", classifierConfig.MinSuperSafeImages,
    )
}

// selectBestFallbackImages: à¹€à¸¥à¸·à¸­à¸à¸ˆà¸²à¸ safe à¸—à¸µà¹ˆà¸¡à¸µ face à¸ªà¸¹à¸‡à¸ªà¸¸à¸” à¹à¸¥à¸° nsfw à¸•à¹ˆà¸³à¸ªà¸¸à¸”
func selectBestFallbackImages(safeResults []classifier.ClassificationResult, count int) []classifier.ClassificationResult {
    // Sort by: face_score DESC, nsfw_score ASC
    sort.Slice(safeResults, func(i, j int) bool {
        scoreI := safeResults[i].FaceScore - safeResults[i].NsfwScore
        scoreJ := safeResults[j].FaceScore - safeResults[j].NsfwScore
        return scoreI > scoreJ
    })

    if len(safeResults) < count {
        return safeResults
    }
    return safeResults[:count]
}
```

#### 5. Update Gallery Upload

```go
// infrastructure/transcoder/gallery_classified.go

func UploadClassifiedGallery(
    ctx context.Context,
    result *ClassifiedGalleryResult,
    remotePrefix string,
    uploader GalleryUploader,
    logger *slog.Logger,
) (superSafeUploaded, safeUploaded, nsfwUploaded int, err error) {

    // Upload super_safe images (for Public SEO)
    superSafeRemote := filepath.ToSlash(filepath.Join(remotePrefix, "super_safe"))
    superSafeCount, _, _ := UploadGallery(ctx, result.SuperSafeDir, superSafeRemote, uploader, logger)

    // Upload safe images (borderline, lazy load)
    safeRemote := filepath.ToSlash(filepath.Join(remotePrefix, "safe"))
    safeCount, _, _ := UploadGallery(ctx, result.SafeDir, safeRemote, uploader, logger)

    // Upload nsfw images (member only)
    nsfwRemote := filepath.ToSlash(filepath.Join(remotePrefix, "nsfw"))
    nsfwCount, _, _ := UploadGallery(ctx, result.NsfwDir, nsfwRemote, uploader, logger)

    logger.Info("three-tier gallery uploaded",
        "remote_prefix", remotePrefix,
        "super_safe", superSafeCount,
        "safe", safeCount,
        "nsfw", nsfwCount,
    )

    return superSafeCount, safeCount, nsfwCount, nil
}
```

#### 6. Update Database Schema

```sql
-- migrations/002_add_three_tier_gallery.sql

ALTER TABLE videos ADD COLUMN gallery_super_safe_count INT DEFAULT 0;
-- gallery_safe_count already exists
-- gallery_nsfw_count already exists

-- Optional: Add flag for public-ready
ALTER TABLE videos ADD COLUMN gallery_public_ready BOOLEAN DEFAULT FALSE;

-- Index for API queries
CREATE INDEX idx_videos_gallery_public ON videos(gallery_public_ready);
```

### API Response Update

```go
// gofiber_starter/domain/dto/video.go

type GalleryInfoDTO struct {
    SuperSafeCount int  `json:"superSafeCount"` // à¸ªà¸³à¸«à¸£à¸±à¸š Public Featured
    SafeCount      int  `json:"safeCount"`      // à¸ªà¸³à¸«à¸£à¸±à¸š Public Lazy Load
    NsfwCount      int  `json:"nsfwCount"`      // à¸ªà¸³à¸«à¸£à¸±à¸š Member
    PublicReady    bool `json:"publicReady"`    // >= 10 super_safe images
}

// API: GET /api/v1/videos/{id}/gallery
type VideoGalleryResponse struct {
    // Public pages à¹ƒà¸Šà¹‰à¸•à¸±à¸§à¸™à¸µà¹‰
    PublicImages []GalleryImageDTO `json:"publicImages"` // super_safe only

    // "à¸”à¸¹à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡" button (lazy load)
    BorderlineImages []GalleryImageDTO `json:"borderlineImages,omitempty"` // safe

    // Member pages à¹ƒà¸Šà¹‰à¸•à¸±à¸§à¸™à¸µà¹‰
    AllImages []GalleryImageDTO `json:"allImages,omitempty"` // super_safe + safe

    // Info
    Info GalleryInfoDTO `json:"info"`
}
```

### Frontend Integration

```tsx
// SEO Worker Article Generation

interface ArticleImages {
  featuredImage: string      // First super_safe image
  galleryImages: string[]    // All super_safe images (for Schema.org)
  lazyImages: string[]       // safe images (hidden from bot)
}

// Use only super_safe for public SEO content
const publicImages = await api.getGallery(videoId, { tier: 'super_safe' })

// Alt text rules (from FRONTEND_IMPLEMENTATION_PLAN.md)
const safeAltText = generateSafeAlt(video.title, video.cast[0].name)
```

### Priority & Impact

| Change | Priority | Impact |
|--------|----------|--------|
| Add `super_safe/` folder | HIGH | Google SafeSearch compliance |
| Update classifier constants | HIGH | Core logic change |
| Update Python classifier | HIGH | Add `is_super_safe` field |
| Update Go wrapper | MEDIUM | Parse new field |
| Update upload logic | MEDIUM | 3 folders instead of 2 |
| Update DB schema | MEDIUM | New counts |
| Backfill existing videos | LOW | Optional, lazy migration |

### Summary

```
BEFORE (Two-Tier):
  /safe/  (< 0.3)  â†’  Public + Member
  /nsfw/  (>= 0.3) â†’  Member only

AFTER (Three-Tier):
  /super_safe/ (< 0.15 + face) â†’  Public SEO (Google Bot sees)
  /safe/       (0.15 - 0.3)    â†’  Public Lazy (hidden from Bot)
  /nsfw/       (>= 0.3)        â†’  Member only
```

**à¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢:** à¸ à¸²à¸à¸—à¸µà¹ˆ Google Bot à¹€à¸«à¹‡à¸™à¸•à¹‰à¸­à¸‡à¸ªà¸°à¸­à¸²à¸” 100%
- à¹„à¸¡à¹ˆà¸¡à¸µ suggestive content
- à¹„à¸¡à¹ˆà¸¡à¸µ arousal expression
- à¹€à¸«à¹‡à¸™à¸«à¸™à¹‰à¸²à¸Šà¸±à¸” à¹„à¸¡à¹ˆà¸¡à¸µ intimate contact
